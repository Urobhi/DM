---
title: "Hiper Parametrización"

date: "2020-09-28"
version: 0.7
output: 
  html_document:
    theme: spacelab
    highlight: monochrome
    df_print: paged
#    toc: true
#    toc_depth: 2

vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css, echo=FALSE}
.tarea {
  padding: 1em;
  border: 2px solid red;
  border-radius: 10px;
  margin-bottom: 10px;
}
```

> ... premature optimization is the root of all evil 
>--- Donald Knuth

> Success is a lousy teacher. It seduces smart people into thinking they can't lose.  
>--- Bill Gates

En este notebook nos empaparemos con detalles de la hiperparametrización con el resultado de las optimizaciones. 

_Preguntas_

* ¿Cuál es el fin de buscar los mejores parámetros?
* ¿Por qué los estamos buscando con optimizaciones del tipo `black-box` y no con, por ejemplo, `descenso por gradiente`? 

Recordamos en la clase pasada la búsqueda de parámetros por `grid search`, hoy veremos rapidamente como funciona la optimización bayesiana con dos ejemplos.

Utilizaremos la librería `mlrMBO`, que nos da muchas opciones.Exploraremos la misma con un ejemplo.


```{r}

rm( list=ls() )
gc()

```

```{r}
library(ggplot2)
library(DiceKriging)
library(mlrMBO)

set.seed(17)
```

Lo primero que tenemos que hacer, es definir una **función objetivo** sobre la que vamos a hacer la búsqueda. Para este caso de ejemplo, usaremos la función seno. Se define a la par, los parámatros sobre los que se va a realizar la búsqueda: **x**, y el espacio de búsqueda: **[3, 13]**. También le definimos con el parámetro `global.opt.value` que busque un máximo, sino por defecto, buscará un mínimo.

```{r}
obj.fun = makeSingleObjectiveFunction(
  name = "Sine",
  fn = function(x) sin(x), 
  par.set = makeNumericParamSet(lower = 3, upper = 13, len = 1)
)
```

Luego empezamos a configurar como va a ser nuestra búsqueda. 

```{r}
ctrl = makeMBOControl()
```

Definimos que vamos a realizar sólo 10 iteraciones.

```{r}
ctrl = setMBOControlTermination(ctrl, iters = 10L)
```

Definimos en función de nuestro objetivo de que forma en que se va a ir realizando la búsqueda.En nuestro caso, le estamos diciendo que estamos buscando sólamente encontrar el optimo, no nos interesa el resto del espacio, y que a cada paso esperamos acercarnos cada vez más. 

```{r}
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(),
                           opt = "focussearch")

lrn = makeMBOLearner(ctrl, obj.fun)
```

Para empezar a buscar necesita una base de puntos, estos mismos los vamos a generar de forma aleatoria, y vamos a generar tan solo 6.

```{r}
design = generateDesign(6L, getParamSet(obj.fun), fun = lhs::maximinLHS)
```

Y simplemente ejecutamos nuestro experimento con todos los parámetros configurados

```{r}
run = exampleRun(obj.fun, design = design, learner = lrn,
                 control = ctrl, points.per.dim = 100, show.info = TRUE)
```

Y vamos a ejecutar una visualización que nos de luz a lo que hizo nuestra optimización paso a paso.

```{r}
plotExampleRun(run, densregion = TRUE, gg.objects = list(theme_bw()), pause = FALSE)

```

A continuación, un ejemplo pero con dos variables.

```{r eval=FALSE}

set.seed(1)
configureMlr(show.learner.output = FALSE)

obj.fun = makeBraninFunction()

ctrl = makeMBOControl(propose.points = 1L)
ctrl = setMBOControlTermination(ctrl, iters = 10L)
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(),
                           opt = "focussearch", opt.focussearch.points = 20L)

lrn = makeMBOLearner(ctrl, obj.fun)
design = generateDesign(10L, getParamSet(obj.fun), fun = lhs::maximinLHS)

run = exampleRun(obj.fun, design = design, learner = lrn, control = ctrl,
                 points.per.dim = 50L, show.info = TRUE)

print(run)

plotExampleRun(run, gg.objects = list(theme_bw()), pause = FALSE)

```


::: {.tarea }
**TAREA**

Ejecute la celda anterior e interprete el paso a paso.

:::

Vamos a analizar de una forma similar nuestra objetivo, cargando previamente las funciones auxiliares para poder trabajar más cómodos:

```{r}

library( "data.table" )
vsemillas <- c(138899, 264463, 383729, 186071, 319433)

Parent_folder     <- dirname(rstudioapi::getSourceEditorContext()$path)
kcarpeta_entregas <- "/entregas/"
kcampos_separador <- "\t"
ds        <- fread(paste0(dirname(Parent_folder),"/datasets/paquete_premium_201906_202001.txt.gz",collapse = ""), header = TRUE, sep= kcampos_separador)


ds[, clase_binaria := ifelse(clase_ternaria == "BAJA+2", "evento", "no_evento")]

# Sacamos la clase ternaria
ds[, c("clase_ternaria") := NULL]

septiembre <- ds[foto_mes == 201909,]
noviembre <- ds[foto_mes == 201911,]
enero <- ds[foto_mes == 202001,]

ds <- NULL
```

Trabajaremos con nuestro ya querido (sólo hasta hoy) `rpart`. Y vamos a analizar nuestro mejor modelo utilizando únicamente el `auc`.

Creemos que el alumnos facilmente podrá cambiar el código para usar la función ganancia.


```{r}
library(rpart)

modelo_rpart <- function (train, test, cp =  0.01, ms = 20, mb = 7, md = 30) { 
  
    modelo <- rpart(clase_binaria ~ ., data = train, 
                    xval=0, 
                    cp=cp, 
                    minsplit=ms, 
                    minbucket=mb, 
                    maxdepth = md )
    
    test_prediccion <- predict(modelo, test , type = "prob")
    roc_pred <-  ROCR::prediction(test_prediccion[,"evento"], test$clase_binaria,
                                  label.ordering=c("no_evento", "evento"))
    auc_t <-  ROCR::performance( roc_pred,"auc")
    unlist(auc_t@y.values)
}

```

Y vamos a trabajar con una muestra, para acortar los tiempos de procesamiento. Una muestra de Septiembre y validando en Noviembre.

**Repito, sólo para realizar las pruebas más rápido**

En nuestro muestreo vamos a **undersamplear** la clase **no_evento**, dejando todos los clase `BAJA+2`.

```{r}
tomar_muestra <- function(datos, resto=10000 ) {
      t <- datos$clase_binaria == "evento"
      r <- rep(FALSE, length(datos$clase_binaria))
      r[!t][sample.int(resto,n=(length(t)-sum(t)))] <- TRUE
      t | r
}

sep_sample <- tomar_muestra(septiembre, resto=20000)

table(septiembre[sep_sample]$clase_binaria)

```

Probamos nuestra mejora en tiempos, calculando el tiempo sin muestreo.

```{r}
t0 <- Sys.time()
r <- modelo_rpart(septiembre, noviembre, cp=0.001)
t1 <- Sys.time()
print(t1-t0)
```

Y con muestreo.

```{r}
t0 <- Sys.time()
r <- modelo_rpart(septiembre[sep_sample], noviembre, cp=0.001)
t1 <- Sys.time()
print(t1-t0)

```

Vamos entonces antes de optimizar los parámetros, a explorar, "a la" grid search, uno de los mismos, para entender en que escenario nos encontramos, buscando el mejor número para `maxdepth`, dejando en el resto de los parámetros fijos.

```{r, eval=FALSE}


resultados <- data.table()

for (v in 1:25) {
    r <- data.table(
      md = v,
      auc = modelo_rpart(septiembre[sep_sample], noviembre, md= v, cp= 0, ms= 1)
    )
    resultados <- rbindlist(list(resultados,r))
}

fwrite(resultados,"C:/Users/saralb/Desktop/UBA2020/work/4_1_resultados.csv.gz")


```

Y hacemos lo que más me gusta, graficar:

```{r}

res <- resultados

ggplot(res, aes(md,auc))  + 
  geom_point()

```

Buscamos el punto máximo:

```{r}
max(res$auc)
res[auc == max(auc), md]

```

Aún así el máximo que nos ofrece nos convence (gráficamente). Para el resultado anterior tuvimos que calcular 25 puntos, veamos si **BO** nos ayuda.

Cómo **BO** busca mínimo, devolveremos nuestra `auc` negativo. 

Configuramos nuestra función objetivo

```{r, eval= FALSE}
set.seed(vsemillas[1])

obj.fun = makeSingleObjectiveFunction(
  name = "max depth",
  fn = function(x) - modelo_rpart(septiembre[sep_sample], noviembre, md= as.integer(x), cp= 0, ms= 1),
  par.set = makeNumericParamSet("pmaxdepth", lower=1L , upper=  25L),
  has.simple.signature = FALSE
)
```

Completamos el experimento, haciendo 10 búsquedas, partiendo de 4 elementos.

```{r, eval= FALSE}
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 10L)
ctrl = setMBOControlInfill(
  ctrl,
  crit = makeMBOInfillCritEI(),
  opt = "focussearch"
)

lrn = makeMBOLearner(ctrl, obj.fun)
design = generateDesign(4L, getParamSet(obj.fun), fun = lhs::maximinLHS)

surr.km <- makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")


run = exampleRun(
  obj.fun,
  design = design,
  learner = surr.km,
  control = ctrl,
  points.per.dim = 25,
  show.info = TRUE
)

saveRDS(run, "C:/Users/saralb/Desktop/UBA2020/work/4_2_OB.RDS")
```


```{r}

run <- readRDS("C:/Users/saralb/Desktop/UBA2020/work/4_2_OB.RDS")
plotExampleRun(run, densregion = TRUE, gg.objects = list(theme_bw()), pause=FALSE)
print(run)

```

Es muy interesante cómo encontró la mejor solución en 1/3 de las iteraciones para un mismo espacio de búsqueda. 

Podemos probar parametrizar otras forma la búsqueda. Repetimos el análisis anterior, pero con otro parámetro, `cp`

```{r}

resultados <- data.table()

for (v in seq(from=0, to=0.001, by=0.00005)) {
    r <- data.table(
      cp = v,
      auc = modelo_rpart(septiembre[sep_sample], noviembre, md= 30, cp= v, ms= 1)
    )
    resultados <- rbindlist(list(resultados,r))
}

fwrite(resultados,"C:/Users/saralb/Desktop/UBA2020/work/4_3_resultados.csv.gz")


```

Y hacemos lo que más me gusta, graficar:

```{r}

res <- fread("C:/Users/saralb/Desktop/UBA2020/work/4_3_resultados.csv.gz")

ggplot(resultados, aes(cp,auc))  + 
  geom_point()

```


Y buscamos el máximo.

```{r}

max(res$auc)
res[auc == max(auc), cp]

```

Pasamos a buscar ahora con un **BO**, cambiando la función objetivo para buscar nuestro nuevo parámetro. Y agregamos el parámetro `minimize` para no tener que poner la ganancia como negativa.

```{r, eval=FALSE}

set.seed(vsemillas[1])

obj.fun = makeSingleObjectiveFunction(
  name = "cp",
  minimize= FALSE,
  fn = function(x) modelo_rpart(septiembre[sep_sample], noviembre, cp= x, ms= 1),
  par.set = makeNumericParamSet("pcp", lower=0L , upper=  0.001L),
  has.simple.signature = FALSE
)

ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 10L)
ctrl = setMBOControlInfill(
  ctrl,
  crit = makeMBOInfillCritEI(),
  opt = "focussearch"
)

lrn = makeMBOLearner(ctrl, obj.fun)
design = generateDesign(5L, getParamSet(obj.fun), fun = lhs::maximinLHS)

surr.km <- makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")


run = exampleRun(
  obj.fun,
  design = design,
  learner = surr.km,
  control = ctrl,
  points.per.dim = 25,
  show.info = TRUE
)

saveRDS(run, "C:/Users/saralb/Desktop/UBA2020/work/4_4_OB.RDS")

```


```{r}
run <- readRDS("C:/Users/saralb/Desktop/UBA2020/work/4_4_OB.RDS")
plotExampleRun(run, densregion = TRUE, gg.objects = list(theme_bw()), pause=FALSE)
print(run)

```

**CHAN!** No llego al optimo. 

_Pregunta_
* ¿Qué falló?

::: {.tarea }
**TAREA**

Adapte los parámetros para poder cubrir mejor el espacio de búsqueda.

:::

Pero como bien sabemos, los óptimos unidimensionales, no necesariamente son los optimos en todas sus dimensiones. Busquemos el optimo de las dos variables conjuntas.


```{r, eval = FALSE }

set.seed(vsemillas[1])

obj.fun = makeSingleObjectiveFunction(
  name = "2 parametros",
  minimize = FALSE,
  fn = function(x) modelo_rpart(septiembre[sep_sample], noviembre, 
                                cp= x$pcp, md= x$pmaxdepth, ms= 1),
  par.set = makeParamSet(
    makeIntegerParam("pmaxdepth",  lower = 1L, upper = 25L),
    makeNumericParam("pcp",  lower=0 , upper=  0.001)
  ),
  has.simple.signature = FALSE
)

```

Y pasamos a realizar la búsqueda, con 20 puntos iniciales y 25 más para buscar.

```{r, eval = FALSE }
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 25L)
ctrl = setMBOControlInfill(
  ctrl,
  crit = makeMBOInfillCritEI(),
  opt = "focussearch"
)

lrn = makeMBOLearner(ctrl, obj.fun)
design = generateDesign(20L, getParamSet(obj.fun), fun = lhs::maximinLHS)

surr.km <-
  makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")

run  <-  mbo(obj.fun, design = design, learner = surr.km, control = ctrl)


saveRDS(run, "C:/Users/saralb/Desktop/UBA2020/work/4_5_OB.RDS")

```

```{r}
run <- readRDS("C:/Users/saralb/Desktop/UBA2020/work/4_5_OB.RDS")
print(run)
```

Tomemos todas las iteraciones para visualizarlas:

```{r}
iter <- as.data.frame(run$opt.path)
```

```{r}

ggplot(iter, aes(x=pcp,y=pmaxdepth,color=y)) +
    scale_color_gradientn(colours = rainbow(10)) +
    geom_point()

```

¿Observaciones?


Pasamos ahora a buscar el mejor juego de parámetros, pero para la función que nos interesa, que es la ganancia y con todos los parámetros. **Con todos los datos** Siguiendo los ejemplos que Gustavo nos comparte en el Dropbox, con ligeros retoques.


```{r, eval=FALSE}

modelo_rpart_ganancia <- function(x) {
  #multiplico para obtener vminbucket
  vminbucket <-  as.integer( round(x$pminbucket * x$pminsplit))
  
  #genero modelo sobre training
  modelo <-  rpart("clase_binaria ~ . ",
                   data= septiembre,
                   xval= 0, 
                   cp=        x$pcp, 
                   minsplit=  x$pminsplit, 
                   maxdepth=  x$pmaxdepth, 
                   minbucket= vminbucket
                  )

  #calculo la ganancia en los datos de testing
  prediccion_test  <- predict( modelo, noviembre, type = "prob")[,"evento"]
  ganancia_test    <- sum( (prediccion_test> 0.025) * 
                           noviembre[, ifelse( clase_binaria=="evento",29250,-750)])

  return( ganancia_test )
}

obj.fun <- makeSingleObjectiveFunction(
            fn   = modelo_rpart_ganancia,
            minimize= FALSE,   #estoy Maximizando la ganancia
            # noisy=    TRUE, # Este parámetro puede ser muy importante, pero tarda. 
            par.set = makeParamSet(
                        makeIntegerParam("pmaxdepth" , lower=3L    , upper=  25L),
                        makeNumericParam("pminbucket", lower=0.001 , upper=   0.5),
                        makeIntegerParam("pminsplit" , lower=1L    , upper= 400L),
                        makeNumericParam("pcp"       , lower=0.0   , upper=   0.001) ),
            has.simple.signature = FALSE
           )


ctrl <-  makeMBOControl()
ctrl <-  setMBOControlTermination(ctrl, iters = 50 )
ctrl <-  setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())

surr.km <-  makeLearner("regr.km", predict.type= "se", covtype= "matern3_2", control = list(trace = FALSE))

run  <-  mbo(obj.fun, learner = surr.km, control = ctrl)
saveRDS(run, paste0(dirname(Parent_folder),"/ajustes/4_6_OB.RDS",collapse = ""))

```

Y analizamos el búsqueda y sus resulados.

```{r}

run <- readRDS("C:/Users/saralb/Desktop/UBA2020/work/4_6_OB.RDS")
run

```

Y armamos el archivo para subir a `Kaggle`

```{r, eval=FALSE}

modelo <- rpart(clase_binaria ~ ., data = noviembre, 
                    xval=0, 
                    cp=0.000117, 
                    minsplit=140, 
                    minbucket=140*0.00102, 
                    maxdepth = 11 )
    
enero_prediccion <- predict(modelo, enero , type = "prob")
    
entrega <-   as.data.table(cbind( "numero_de_cliente" = enero[, numero_de_cliente],  "prob" =enero_prediccion[, "evento"]) )

entrega[  ,  estimulo :=  as.integer( prob > 0.025)]

fwrite(entrega[, c("numero_de_cliente", "estimulo")], 
       "C:/Users/saralb/Desktop/UBA2020/entregas/arbol_bo.csv")

```

