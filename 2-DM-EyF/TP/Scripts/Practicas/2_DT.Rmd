---
title: "Primer paso: Decision Tree"

date: "2020-09-14"
version: 0.7
output: 
  html_document:
    theme: spacelab
    highlight: monochrome
    df_print: paged
#    toc: true
#    toc_depth: 2

vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css, echo=FALSE}
.tarea {
  padding: 1em;
  border: 2px solid red;
  border-radius: 10px;
  margin-bottom: 10px;
}
```

> Even if I knew that tomorrow the world would go to pieces, I would still plant my apple tree.
> --- Martin Luther 

En la clase pasada, empezamos a explorar los datos que recibimos para tratar de predecir el churn. Entendimos la estructura de los mismos y sobre algunas variables de baja cardinalidad armamos reglas simples que nos permitían encontrar clientes a los cuales estimular.

Esta forma de trabajar con estas variables no es práctica para trabajar con variables que podríamos considerar **continuas**. 

Para estas últimas vamos a _discretizarlas_ cortandolas en pedazos que podamos trabajar a la mismas de una manera similar a la que trabajamos las variables de baja cardinalidad. 

Pongamos manos a la obra y sigamos trabajando únicamente con el mes de noviembre (por ahora). Subamos los datos a memoria.

```{r}
# Limpiamos todo antes de empezar a trabajar
rm( list=ls() )
gc()

Parent_folder     <- dirname(rstudioapi::getSourceEditorContext()$path)
kcampos_separador <- "\t"
noviembre        <- fread(paste0(dirname(Parent_folder),"/datasets/paquete_premium_201906_202001.txt.gz",collapse = ""), header = TRUE, sep= kcampos_separador)[foto_mes == 201911,] 






```

La forma más simple de **cortar** una variable continua es en 2 intervalos. O sea definir un **punto de corte** y generar dos intervalos, con el conjunto inferior y superior de datos. 

Probemos para una variable que consideremos importante, y un punto de corte 

```{r}

variable <- "mcuentas_saldo"
punto_corte <- 0

```

Y construimos una variable que nos identifique el intervalo inferior y superior.

```{r}
noviembre[,bin:= ifelse(noviembre[, ..variable]  <= punto_corte, "inf", "sup")]

head(noviembre[,c(variable, "bin", "clase_ternaria"), with=FALSE], 50)
```

Ya _"super"_ acostumbrados a trabajar con la función `dcast`, armamos una tabla cruzada.

```{r}


ds_bin_target <- dcast(noviembre, bin ~ clase_ternaria, 
                       length, 
                       value.var = "clase_ternaria")
# Sacamos la variable bin
noviembre$bin <- NULL

ds_bin_target
```

Nos resta calcular la _jugosa_ ganancia de nuestra de cada intervalo.

```{r}
ds_bin_target[, ganancia := 29250 * `BAJA+2` - 750 * (CONTINUA + `BAJA+1`)]
ds_bin_target
```

Bien. bien. bien. **NO es un buen punto de corte**. Me genera mucha perdida, o muchísima perdida. ¡Pero bueno! sólo probamos **1** punto de corte, seguramente con otros podremos en algunos de los **intervalos** encontrar ganancia positiva.

_Pregunta:_

¿Es posible encontrar ganancia positiva en los dos intervalos que cortan un variable?

Pero repetir todos los pasos anteriores es muy engorroso. Creemos una función para simplificar nuestra vida. La misma nos va a devolver un vector con las ganancias de los intervalos inferiores y superiores.

```{r}
fganancia_corte <- function(pdatos, 
                            pvariable, 
                            ppunto_corte, 
                            ptarget = "clase_ternaria")
{
    indice <- pdatos[, get(pvariable)] <= ppunto_corte
    
    
    
    intervalo_inferior <- sum(
                            ifelse(pdatos[indice, get(ptarget)] == "BAJA+2",
                                     29250, -750))
    intervalo_superior <- sum(
                            ifelse(pdatos[!indice, get(ptarget)] == "BAJA+2",
                                     29250,-750))
    return(c(intervalo_inferior, intervalo_superior))
}

```

Chequeamos llegar al mismo resultado para el mismo **punto de corte**, ya que no escribimos la funci?n utilizando otras t?cnicas.

```{r}
fganancia_corte(noviembre, "mcuentas_saldo", 0)
fganancia_corte(noviembre, "mcuentas_saldo", -10000)
fganancia_corte(noviembre, "mcuentas_saldo", -20000)
fganancia_corte(noviembre, "mcuentas_saldo", -30000)
fganancia_corte(noviembre, "mcuentas_saldo", -40000)
fganancia_corte(noviembre, "mcuentas_saldo", -50000)
fganancia_corte(noviembre, "mcuentas_saldo", -60000)
fganancia_corte(noviembre, "mcuentas_saldo", -70000)
fganancia_corte(noviembre, "mcuentas_saldo", -80000)
fganancia_corte(noviembre, "mcuentas_saldo", -90000)
```

¡Listo! ya podemos cortar una variable rápidamente. Probemos más **puntos de corte**.

```{r}

fganancia_corte(noviembre, "mcuentas_saldo",  -100000)
fganancia_corte(noviembre, "mcuentas_saldo",  -50000)
fganancia_corte(noviembre, "mcuentas_saldo",  -20000)
fganancia_corte(noviembre, "mcuentas_saldo",  -10000)
fganancia_corte(noviembre, "mcuentas_saldo",  10000)

```

Bien, ya tenemos algunos intervalos en los que la ganancia es positiva. Pero nos encontramos frente a la pregunta: _¿Cuál es el mejor punto de corte que genere un intervalo de mayor ganancia?_

Algunos alumnos recordarán algunos algoritmos matemáticos de búsquedas de optimos. Pero, no sabemos nada de la función ganancia en relación a nuestra variable. Esto nos dificulta mucho usarlos.

Un camino más simple es probar todos los posibles valores. Que trabaje la computadora, que encuentre por fuerza bruta el mejor punto de corte probando todos los posibles valores que se encuentra en una variable.

Pero nos encontramos frente a un dificultad. El tiempo. ¿Nos dará la vida para trabajar de esta manera?. Midamos cuanto tarda buscar un punto de corte. Para esto vamos a medir el tiempo de 10 puntos de corte.

```{r}
tiempos <- c()
for (i in 1:10)
{
    punto <- runif(1, 
                   min=min(noviembre[,mcuentas_saldo]),
                   max=max(noviembre[,mcuentas_saldo]))
    
    t0   <-  Sys.time()
    fganancia_corte(noviembre, "mcuentas_saldo",  punto)
    t1   <-  Sys.time()
    tiempos <- cbind(tiempos, as.numeric( t1 - t0, units = "secs"))
}
mean(tiempos)
```

Bueno, cada punto de corte que probamos sólo nos tarda una fracción de segundo. No parece mucho. Veamos cuanto nos tardaría encontrar el mejor corte para nuestra variable `mcuentas_saldos`.

```{r}
cantidad_cortes <- length(unique(noviembre[,mcuentas_saldo]))
cantidad_cortes
```

```{r}
cantidad_cortes*mean(tiempos)
```

Bueno, eso parecen muchos segundos. Son **muchas horas**. Es inviable este camino. ¿O tan solo es inviable la forma en que se esta forma de buscar el **punto de corte**? 

Si repensamos el algoritmo y calculamos _a la vez_ todos los posibles puntos de corte podremos bajar considerablemente los tiempos. Veamos con detalle la siguiente función.

```{r}
fmejor_corte <-  function(pdatos, pvariable, ptarget = "clase_ternaria") 
{
    # Contamos la cantidad de BAJA+2 y la ganancia total, suma de mandar todos los estimulos
    baja_2    <-  sum(pdatos[, get(ptarget)] == "BAJA+2" )
    ganancia_total <-  29250*baja_2 - 750*(nrow(pdatos) - baja_2)
    
    # Me quedo solamente con la variable a cortar y la clase
    ds    <-  pdatos[,  c(pvariable, ptarget),  with=FALSE ]
    
    # Ordeno los datos seg?n variable a cortar
    ds <- ds[ order(get(pvariable)),]
    
    # Calculo las ganancias acumuladas, o lo que es lo mismo, la ganancia 
    # del intervalo inferior ( usando un punto de corte <= )
    ds[, inferior := cumsum( 
      ifelse( ds[, get(ptarget)] == "BAJA+2", 29250, -750 ))]
    
    # Luego calculo la ganancia del otro intervalo como la diferencia entre la 
    # ganancia total menos la ganancia del intervalo inferior
    ds[, superior := ganancia_total - inferior]
    
    # Buscamos cual es la m?xima ganancia posible para ese registro
    ds[, max_gan := ifelse(inferior >= superior, inferior, superior)]
    
    #Devolvemos el registro con la maxima ganancia
    ds[max_gan == max(max_gan), c(pvariable, "inferior", "superior"), with=FALSE]
}

```

Y la probamos con la variable que estuvimos trabajando.

```{r}
fmejor_corte(noviembre, "mcuentas_saldo")
```

Vemos que encontramos una mejor ganancia de lo que buscamos a mano y no tardó nada de tiempo. 

**Lección**: La forma en que se hacen las cosas influye mucho.

Aunque parezca un mero ejercicio, estas cosas se necesitan en la vida real. 


_Preguntas:_

* ¿Qué hacemos con las variables con datos faltantes?

* Anteriormente se corto la variable solo en dos intervalos. ¿Cómo podríamos cortalos en más intervalos? ¿Cuál es el costo? ¿Se recomienda?

::: {.tarea }
**TAREA AVANZADA**

Mejore la función permitiendo el uso de datos faltantes.

Si dos registros tienen el mismo valor, ¿Hay una mejor forma de ordenarlo? ?Cómo puede trabajar este problema?

:::

Probemos con otra variable:

```{r}
fmejor_corte(noviembre, "Visa_mfinanciacion_limite")
```

Vemos que esta última variable es mucho mejor, ya que nos da una ganancia mucho mejor. 

Sería interesante probar todas las variables y buscar la mejor.

::: {.tarea }
**TAREA**

Escriba una rutina en R que busque la mejor de las variables. Si no mejor? la función dada, pruebe solamente con un vector de variables sin valores ausentes.

:::

Una vez que tenemos la mejor variable que mejor corta a nuestro conjunto de datos nos nace la pregunta: ¿Podemos cortar esos intervalos por otra variable?

Y de esta forma, a través de este concepto iterativo llegamos a la necesidad de construir un árbol de decisión. Para refrescar el funcionamiento de los árboles de decisión, miremos el sitio [R2D3](http://www.r2d3.us/), los apartados:

* [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
* [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)

_Preguntas_

- ¿Recuerda la proporción de `BAJA+2` presente?
- ¿En que puede afectar este desbalanceo al algoritmo?


Empecemos construyendo nuestro el primer árbol y veamos la salida del mismo. Seguiremos trabajando con nuestro mes de `Noviembre`, armando un árbol simple.

```{r}

library(rpart)

modelo   <-  rpart( clase_ternaria ~ .,   data = noviembre,  xval=0 )

```

Veamos el modelo que nos devolvió.

```{r}

modelo

```

Veamos este visualizando el árbol resultante de manera más clara (?).

```{r}
library( "rpart.plot" )

prp( modelo, extra=101, digits=5, branch=1, type=4, varlen=0, faclen=0 )
```

Sentemonos a criticar esta horrible visualización, donde no queda expresado de forma clara donde terminan los grandes volumenes de clientes en cada uno de los nodos. Dedicamos unos instantes en entenderlo nosotros revisando las reglas nuevamente.

Sería muy tentador poder jugar con los datos del modelo, por esto mismo, vamos a pasar el modelo a una tabla sobre la que podamos jugar. Construimos la siguiente función, y entendemos a través de la experiencia, que R, no es el lenguaje más lindo para escribir facilmente código. (¡maldigo la invención de los rownames!)

```{r}

ftabla_reglas <- function (pmodelo, pdatos, ptarget = "clase_ternaria") {

  # Tomamos la columna con el target
  target_vector <- pdatos[, get(ptarget)]
  # Tomamos las clases de nuestro target
  classes <- unique(target_vector)
  
  # Tomamos las posicion de las reglas que aplican a los registro de nuestro ds,
  # y obtenemos las reglas
  
  row_leaf <- unique(pmodelo$where)
  row_name_leaf <- as.integer(rownames(pmodelo$frame[row_leaf,]))
  rules <- path.rpart(pmodelo, row_name_leaf, pretty = 0, print.it=FALSE)
  rules_concat <- lapply(rules, 
                         function(y) paste0(tail(y, n=-1), collapse = " & "))
  leaves <- data.table(row_frame = row_leaf, 
                       rules = rules_concat)
  setkey(leaves,row_frame)
  
  # Relacion target ~ hojas
  leaves_target <- dcast(
    
    data.table(
      target=target_vector, 
      leaf = pmodelo$where), 
    
    leaf ~ target, length, 
    value.var = "target")
  
  setkey(leaves_target, leaf) 
  
  # Juntamos todo
  leaves_target <- leaves_target[leaves,nomatch=0]
  
  # Sumamos algunas columnas calculadas
  colnames(leaves_target[,classes,with=FALSE])[apply(leaves_target[,classes,with=FALSE],1,which.max)]
  # Clase mayoritaria
  leaves_target[, 
                y:=colnames(
                    leaves_target[,classes,with=FALSE]
                  )[
                    apply(leaves_target[,classes,with=FALSE],1,which.max)]
                ]
  
  # Cantidad clase mayoritaria
  leaves_target[, y_n:=unlist(apply(leaves_target[,classes,with=FALSE],1,max))]
  # Cantidad de elementos de la hoja
  leaves_target[, n := unlist(Reduce(function(a,b) Map(`+`,a,b), .SD)), .SDcols=classes]
  # Perdida
  leaves_target[, loss := n - y_n]
  
  # Return
  leaves_target
}

```

El fin de la función es extraer la información del modelo de la estructura de datos que nos devuelve la librería `rpart`

Aplicamos nuestra función a los datos generados:

```{r}

resultados <- ftabla_reglas(modelo, noviembre)
resultados
# View(resultados) # No siempre visualiza las reglas por ser un string largo.

```


_Pregunta:_

* ?Con qué criterio eligió la clase de cada rama, y por ente, determino la clasificación de los registros?

Vemos que el algoritmos sólo eligió en una hoja la clase `BAJA+2`. Calculemos la ganancia que tiene cada rama. 

```{r}

resultados[, ganancia:= `BAJA+2`*29250 - 750*(CONTINUA + `BAJA+1`)]
resultados

```

**CHAN**. Si hicieramos caso a las clases del árbol, tendríamos una ganancia de:

```{r}

resultados[y == "BAJA+2", .(ganancia=sum(ganancia), enviados=sum(n), sevan=sum(`BAJA+2`))]

```

Pero, si tomamos todas las ramas que nos dieron ganancia positiva:

```{r}

resultados[ganancia > 0, .(ganancia=sum(ganancia), enviados=sum(n), sevan=sum(`BAJA+2`))]

```

Wow!! Estabamos dejando mucho plata sobre la mesa! **MUCHA** Veamos cómo clasificó a las hojas que nos dan ganancia positiva:

```{r}

resultados[ganancia > 0, .N, by=y]

```

Listo, ¡no podemos confiar en la clase! **Otra importante lección de vida** 

Entendemos que la diferencia se debe a la cantidad de casos en cada nodo y la proporción de `BAJA+2` en cada hoja. Vamos a trabajar con una nueva clase, una binaria:

BAJA+2 = evento
BAJA+1 o CONTINUA = noevento 


```{r}
noviembre[, clase_binaria := ifelse(clase_ternaria == "BAJA+2", "evento", "noevento")]

# Sacamos la clase ternaria
noviembre[, clase_ternaria:= NULL]
```

Con el nuevo `target`, pasamos a ajustar nuevamente.

```{r}

modelo2   <-  rpart( clase_binaria ~ .,   data = noviembre, xval=0 )
modelo2

```

**CHAN**! No nos abre el árbol!!

Esto se debe a los parámetros que le pasamos, la _complejidad_ es muy alta. Veremos en otra clase las parametría del árbol, ahora simplemente bajaremos el parámetro *cp* y con esto obtendremos un árbol que separe las clases.

```{r}

modelo2   <-  rpart( clase_binaria ~ .,   data = noviembre,   cp=0.001,  xval=0 )

modelo2


```

Vemos que nuestro nuevo árbol es mucho más grande y tiene muchas más _hojas_.

Ahora aplicamos nuestra función para trabajar con los resultados: 

```{r}

resultados2 <- ftabla_reglas(modelo2, noviembre, "clase_binaria")
resultados2

```

Revisemos rápidamente la ganancia que obtenemos por clase *evento*

```{r}

resultados2[, ganancia:= evento*29250 - 750*noevento]
resultados2[y == "evento", .(ganancia=sum(ganancia), enviados=sum(n), sevan=sum(evento))]

```

La ganancia total si tomamos todos los nodos donde la misma es positiva:

```{r}

resultados2[ganancia > 0, .(ganancia=sum(ganancia), enviados=sum(n), sevan=sum(evento))]

```

**WAIT**! Subió mucho la ganancia, pronto tendremos que controlar el *overfitting*, pero no ahora. veamos a que tipo de clases pertenece esa ganancia:

Ya no sosteniendo más esta forma de trabajar, vamos a calcular las probabilidades por hoja de que se vaya en 2 meses un cliente.

```{r}

resultados2[, c("p_evento","p_noevento"):= list(evento/n, noevento/n) ]
resultados2

```

Graficamos para darnos una idea sobre la distribución de las probabilidades. 

Primero veamos la distribución de la variable probabilidad de salida.

```{r}
library(ggplot2)

ggplot(resultados2, aes(x=p_evento)) +
     geom_density(aes(weights=y_n))
```

Vemos que se concentra en valores cercanos a **0**. 

Luego separamos las distribuciones de probabilidad para los casos de `evento` y de `no_evento`



```{r}
ggplot(resultados2, aes(x=p_evento)) +
    facet_grid(vars(y), scales = "free_y") +
     geom_density(aes(weights=y_n))

```

_Preguntas:_
* ¿Qué interpreta de estas densidades? 
* ¿Separa bien el árbol? 

Definimos `punto de corte` a la probabilidad mínima que tiene que tener un elemento sobre esa clase para considerarla de la misma. 

Mirando el gráfico anterior, ¿Qué punto de corte sugiere?

Con esta definición y las probabilidades obtenidas, nos planteamos evaluar cuál debe ser nuestro punto de corte, y de esta manera dejar de depender de evaluar las _hojas_ del árbol.

Para esto, ordenamos nuestras probabilidades de salida de forma inversa. Estoy puede parecer contra intuitivo, pero nos ayuda a trabajar con los intervalos inferiores.

```{r}
resultados3 <- resultados2[order(-p_evento),]
```

Como más de una _hoja_ de salida, comparte la misma probabilidad, las agrupamos según su probabilidad.

```{r}
resultados3 <- resultados3[,.(evento=sum(evento), 
                              noevento=sum(noevento),
                              n=sum(n),
                              ganancia=sum(ganancia)),by=p_evento]
resultados3
```

Luego, calculamos la ganancia acumulada. Los primero registros, los correspondientes a los probabilidades más altas, son de esperar que tengan una mayor ganancia. Luego esta debería caer hasta el piso, que sería el costo de enviar a todos los clientes `premium` un estímulo.

```{r}
resultados3[, gan_acum:=cumsum(ganancia)]
resultados3
```

Grafiquemos esta última variable, para entender el problema que estamos enfrentando.

```{r}
ggplot(resultados3, aes(x=p_evento ,y=gan_acum)) +
     scale_x_reverse() +
     geom_line(size=1)

```

![Zoom por favor](https://i.imgflip.com/17bdt9.jpg)

Vamos a focalizarnos sólo donde se encuentra el **pico** de la ganancia. Para esto busquemos este pico.

```{r}
max_ganancia <- max(resultados3$gan_acum)
max_ganancia
```

Y a cúal **punto de corte** corresponde.

```{r}
max_punto_corte <- resultados3[gan_acum == max_ganancia, p_evento ]
max_punto_corte
```

**Wait**, ese no es el **punto de corte** teórico que vimos en clase. Tratemos de ver si el gráfico nos da algo de luz sobre porque es distinto el **punto de corte**.

```{r}
ggplot(resultados3, aes(x=p_evento,y=gan_acum)) +
    scale_x_reverse() +
    geom_line(size=1) + 
    xlim(c(0.00000005,0.2)) + 
    ylim(c(0,8000000)) + 
    geom_vline(xintercept = max_punto_corte) 
    # annotate("text",x=0.01,y=100000, label = c(l))

```

Un ojo relativamente entrenado, puede sospechar que faltan valores de probabilidades entre nuestro pico y el valor _teórico_. Se puede calcular de esta forma si nuestro 0.025 es un buen punto de corte.

```{r}
resultados3[p_evento >= 0.025, max(gan_acum) ]
```

Es el mismo valor. Con este análisis, más el respaldo teórico, vamos a tomar como `punto de corte` 0.025.

Existen más formas de medir la calidad del modelo a través de las probabilidades que nos entrega. A nivel global podemos usar `AUC` (área bajo la curva ROC), que nos muestra el comportamiento global de la performance del modelo. 

Vamos a trabajar de forma casera para el cálculo de los indicadores **básicos**. Más adelante usaremos librerías.

Para los indicadores los vamos a contruir en funci?n de la [Matriz de confusión](https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion). Donde, vamos a contar con una matriz por cada **punto de corte**.

Para esto, vamos a sumar las variables según el nombre de matriz de confusión, nuevamente: siempre con la referencia del punto de corte.
```{r}

resultados3[, c("evento_acum","noevento_acum"):=list(cumsum(evento),cumsum(noevento))]
total_evento <- resultados3[,sum(evento)]
total_noevento <- resultados3[,sum(noevento)]

resultados3[, c("evento_restantes","noevento_restantes"):=list(total_evento - evento_acum,total_noevento - noevento_acum )]

resultados3[,tp:=evento_acum]
resultados3[,tn:=noevento_restantes]
resultados3[,fp:=noevento_acum]
resultados3[,fn:=evento_restantes]

resultados3
```

Ya con nuestras **matrices de confusión** armadas, pasaremos a estudiar una de las más importantes:**la curva ROC**. 

Para esta vamos a necesitar un par de variables más:

```{r}

resultados3[,tpr:=(tp/(tp+fn))]
resultados3[,fpr:=(fp/(fp+tn))]

```

La **curva ROC** es la curva resultante en la relación del **ratio de falsos positivos** y **ratio de verdaderos positivos**.

```{r}

ggplot(resultados3, aes(x=fpr,y=tpr)) + 
  geom_abline(intercept=0,slope=1) +
  geom_line(lwd=1) 

```

Como es muy complejo reflejar en palabras una curva, se suele calcular el área bajo su curva, **auc** y reflejar ese valor como métrica de la calidad del modelo.

_Preguntas:_
* ¿**AUC** es una métrica global o local?
* ¿Pueden dos curvas distintas tener un mismo valor de **AUC**?

Calculamos su área a pulmón ... nah usamos una librería.

```{r}
# install.packages("geometry")
library(geometry)

x <- c(resultados3$fpr,1)
y <- c(resultados3$tpr, 0)
polyarea(x, y)

```

Finalmente, luego de un largo trabajo, tenemos el famoso **AUC**. Larga vida a las librerías!

Veamos ahora, una métrica más:

**Accuracy** (pero de acuerdo al punto de corte):

```{r}

resultados3[, acc:= ((tp + tn)/(tp+tn+fp+fn))]

```

Desde ahora, el accuracy no es una métrica, se gradua, y se transforma en curva.

```{r}

ggplot(resultados3, aes(x=p_evento,y=acc)) + 
  geom_line(lwd=1) +
  geom_vline(xintercept = 0.025, linetype="dotted")

```

_Preguntas:_
* ¿Cuál es el _cutoff_ optimo según la curva de _accuracy_?
* ¿Por qué es una curva tan buena?
* ¿Se ajusta a nuestra necesidad de negocio?


::: {.tarea }
**TAREA**

Construya la curva correspondiente al F1 Score.

La métrica F1, es criticado por dar un mismo peso a `recall` y al `precision`. Por esto mismo, a alguien se le ocurrió el F-Beta. Construya esta última para varios Betas. ¿Hay algún Beta que tenga un **punto de corte** similar al nuestro?

:::

Por úlitmo, veamos como construir un archivo de estimulos para subir a _Kaggle_ resultado de scorear un árbol.

```{r}

# Cargo el mes de Enero de 2020
enero <- fread(paste0(kcarpeta_datasets, 
                        karchivo_entrada_7meses_zip,
                        collapse = ""), header=TRUE, sep=kcampos_separador
                        )[foto_mes == 202001,] 

prediccion_202001      <- predict( modelo2, enero, type = "prob")

entrega <-   as.data.table(cbind( "numero_de_cliente" = enero[, numero_de_cliente],  "prob" =prediccion_202001[, "evento"]) )

entrega[  ,  estimulo :=  as.integer( prob > 0.025)]

fwrite(entrega[, c("numero_de_cliente", "estimulo")], paste(kcarpeta_entregas, "arbol_simple.csv", collapse = ""))

```

::: {.tarea }
**TAREA**

Usted tiene experiencia en el uso de árboles de decisión. Use esos conocimientos en la parametrización para buscar el mejor árbol y suba a _Kaggle_ sus estimulos.

:::

